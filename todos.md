Proposed Volume Configuration

volumes:
    # Core project directories
    - ./citibike:/opt/airflow/citibike
    - ./config:/opt/airflow/config
    - ./dags:/opt/airflow/dags
    - ./dbt_transformations:/opt/airflow/dbt_transformations
    - ./plugins:/opt/airflow/plugins
    - ./logs:/opt/airflow/logs
    # Scripts directory (new)
    - ./scripts:/opt/airflow/scripts
    # Still need setup.py for pip install -e .
    - ./setup.py:/opt/airflow/setup.py
    - ./requirements.txt:/opt/airflow/requirements.txt

Required Changes

1. Move `generate_dbt_profile.py` to `scripts/` - As you mentioned, this makes sense organizationally
2. Update custom entrypoint path - Change line 26 in Dockerfile from `python generate_dbt_profile.py` to `python scripts/generate_dbt_profile.py`
3. Create `scripts/` directory - For any container-execution scripts
4. Document selective volume mapping in README
5. Create .dockerignore with:
  .git/
  .venv/
  logs/
  *.egg-info/
  __pycache__/
  airflow.cfg
  webserver_config.py

In Dockerfile init script:
  # Remove the flag check - just run every time
  echo "üöÄ Installing citibike package from volume mount..."
  pip install -e .

  echo "üìù Generating dbt profile if needed..."
  if [ ! -f dbt_transformations/profiles.yml ]; then
      python scripts/generate_dbt_profile.py
  fi


Scripts Analysis

  Should Move to scripts/:

  1. generate_dbt_profile.py ‚úÖ Move

  - Purpose: Container initialization script
  - Current paths:
    - Writes to dbt_transformations/profiles.yml (line 62)
  - Required change: Update write path to ../dbt_transformations/profiles.yml

  2. create_tables.py ‚úÖ Move

  - Purpose: Administrative/deployment script
  - Current paths:
    - Reads from sql/ddl/templates/**/*.sql (line 9)
  - Required change: Update glob path to ../sql/ddl/templates/**/*.sql

  Should NOT Move:

  3. setup.py ‚ùå Keep in Root

  - Purpose: Python package definition
  - Reason: Must stay at package root for pip install -e . to work

  4. generate_holidays.py ‚ùå Keep in Root (or archive)

  - Purpose: One-time data generation (already completed)
  - Current paths: Writes to dbt_transformations/seeds/holidays.csv (line 34)
  - Reason: Documentation script, already run, rarely used

  5. webserver_config.py ‚ùå Ignore (Airflow-generated)

  - Purpose: Airflow webserver configuration
  - Reason: Auto-generated by Airflow, will be eliminated by selective volumes

  Required Changes

‚è∫ For scripts/generate_dbt_profile.py:

  # Line 62: Change from:
  with open('dbt_transformations/profiles.yml', 'w') as f:

  # To:
  with open('../dbt_transformations/profiles.yml', 'w') as f:

  For scripts/create_tables.py:

  # Line 9: Change from:
  TEMPLATE_PATHS = glob("sql/ddl/templates/**/*.sql", recursive=True)

  # To:  
  TEMPLATE_PATHS = glob("../sql/ddl/templates/**/*.sql", recursive=True)


  Dockerfile Changes:

  # Line 26: Change from:
  python generate_dbt_profile.py

  # To:
  python scripts/generate_dbt_profile.py

  Summary

  Move to scripts/:
  - generate_dbt_profile.py (2 path updates needed)
  - create_tables.py (2 path updates needed)

  Keep in root:
  - setup.py (required for pip install)
  - generate_holidays.py (one-time script, rarely used)

  Auto-eliminated:
  - webserver_config.py (Airflow-generated, won't appear with selective volumes)
  - airflow.cfg (Airflow-generated, won't appear with selective volumes)


Nice to have:
In your docker-compose.yaml update, you might want to also update the airflow-init service volume mount:

# Current (line 251):
- ${AIRFLOW_PROJ_DIR:-.}:/sources

# Should become:
- ./logs:/sources/logs
- ./dags:/sources/dags
- ./plugins:/sources/plugins

